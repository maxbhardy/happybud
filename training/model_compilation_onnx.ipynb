{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation du modèle avec ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Préparation et compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Créé à l'aide de ChatGPT\n",
    "import torch\n",
    "\n",
    "class ReshapeToBatchChannelFirst(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReshapeToBatchChannelFirst, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure the input is of shape (224, 224, 3)\n",
    "        #assert x.dim() == 3 and x.shape[-1] == 3, \"Input must be (224, 224, 3)\"\n",
    "        \n",
    "        # Permute dimensions from (H, W, C) to (C, H, W)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        \n",
    "        # Add a batch dimension at the beginning: (1, C, H, W)\n",
    "        x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "x = torch.rand(224, 224, 3)  # Example input\n",
    "layer = ReshapeToBatchChannelFirst()\n",
    "output = layer(x)\n",
    "print(output.shape)  # Expected: torch.Size([1, 3, 224, 224])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.4975]],\n",
      "\n",
      "         [[-2.4878]],\n",
      "\n",
      "         [[-2.4833]]]])\n"
     ]
    }
   ],
   "source": [
    "# Créé à l'aide de ChatGPT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FixedNormLayer(torch.nn.Module):\n",
    "    def __init__(self, scale: torch.Tensor, mean: torch.Tensor, std: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mean (torch.Tensor): Precomputed mean for normalization.\n",
    "            std (torch.Tensor): Precomputed standard deviation for normalization.\n",
    "        \"\"\"\n",
    "        super(FixedNormLayer, self).__init__()\n",
    "        self.register_buffer(\"mean\", mean[:, None, None])\n",
    "        self.register_buffer(\"std\", std[:, None, None])\n",
    "        self.register_buffer(\"scale\", scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (self.scale * x - self.mean) / self.std\n",
    "\n",
    "# Example usage\n",
    "mean = torch.tensor([0.5, 0.5, 0.5])  # Example mean for 3 channels\n",
    "std = torch.tensor([0.2, 0.2, 0.2])   # Example std for 3 channels\n",
    "scale = torch.tensor([1 / 256])\n",
    "layer = FixedNormLayer(scale, mean, std)\n",
    "\n",
    "# Test with a sample input\n",
    "x = torch.rand(1, 3, 1, 1)  # Example input\n",
    "output = layer(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créé à l'aide de ChatGPT\n",
    "class InferenceModel(torch.nn.Module):\n",
    "    def __init__(self, model, scale, mean, std):\n",
    "        super(InferenceModel, self).__init__()\n",
    "        self.preprocess = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"reshape\", ReshapeToBatchChannelFirst()),\n",
    "                    (\"normalize\", FixedNormLayer(scale, mean, std)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.model = model  # The main model\n",
    "        self.postprocess = torch.nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preprocess(x)  # Apply reshaping and normalization\n",
    "        x = self.model(x)  # Pass to the main model\n",
    "        return self.postprocess(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification du modèle pour avoir le bon nombre de sortie dans la dernière couche et\n",
    "pour calculer le softmax sur les sorties du modèle pour avoir directement les probabilités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ViT\n",
    "num_labels = 11  # Get number of labels (e.g., 8)\n",
    "\n",
    "model = torchvision.models.vit_b_16(weights=\"IMAGENET1K_V1\")  # Load a pretrained model\n",
    "model.heads.head = torch.nn.Linear(model.heads.head.in_features, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=11, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"models/tomato_model_2025_02_28_v2.pt\",\n",
    "        map_location=device,\n",
    "        weights_only=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0214, 0.6055, 0.2212, 0.0180, 0.0034, 0.0022, 0.0020, 0.0812, 0.0186,\n",
       "         0.0066, 0.0199]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create inference model\n",
    "scale = torch.tensor([1 / 256])\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "inference_model = InferenceModel(model, scale, mean, std)\n",
    "inference_model.eval()\n",
    "\n",
    "test = torch.randn(224, 224, 3)\n",
    "inference_model(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportation du modèle, en incluant un tenseur aléatoire pour fournir la bonne taille de\n",
    "tenseur en entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/onnx/_internal/fx/passes/readability.py:52: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  new_node = self.module.graph.get_attr(normalized_name)\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/fx/graph.py:1586: UserWarning: Node preprocess_normalize_scale target preprocess/normalize/scale preprocess/normalize/scale of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/fx/graph.py:1586: UserWarning: Node preprocess_normalize_mean target preprocess/normalize/mean preprocess/normalize/mean of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/fx/graph.py:1586: UserWarning: Node preprocess_normalize_std target preprocess/normalize/std preprocess/normalize/std of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/onnx/_internal/fx/onnxfunction_dispatcher.py:503: FutureWarning: 'onnxscript.values.TracedOnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  self.param_schema = self.onnxfunction.param_schemas()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 37 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "source": [
    "torch_input = torch.randn(224, 224, 3)\n",
    "onnx_program = torch.onnx.dynamo_export(inference_model, torch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces warnings ne sont probablement pas grave, selon cette\n",
    "[source](https://github.com/pytorch/pytorch/issues/144331)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(\"models/tomato_model_2025_02_28_v2.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportation du modèle en format .ort pour l'exécution sur mobile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting models with optimization style 'Fixed' and level 'all'\n",
      "Converting optimized ONNX model /home/maxime/Documents/Code/happybud/training/models/tomato_model_2025_02_28_v2.onnx to ORT format model /home/maxime/Documents/Code/happybud/training/models/tomato_model_2025_02_28_v2.ort\n",
      "Converted 1/1 models successfully.\n",
      "Generating config file from ORT format models with optimization style 'Fixed' and level 'all'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 10:02:40,959 ort_format_model.utils [INFO] - Created config in /home/maxime/Documents/Code/happybud/training/models/tomato_model_2025_02_28_v2.required_operators.config\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from onnxruntime.tools import convert_onnx_models_to_ort as convert_onnx\n",
    "\n",
    "convert_onnx.convert_onnx_models_to_ort(\n",
    "    pathlib.Path(\"models/tomato_model_2025_02_28_v2.onnx\"),\n",
    "    output_dir=pathlib.Path(\"models\"),\n",
    "    optimization_styles=[convert_onnx.OptimizationStyle.Fixed],\n",
    "    target_platform=\"arm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Validation de l'exécution du modèle avec ONNX runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"models/tomato_model_2025_02_28_v2.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline fait sans pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_pipeline(image_path, dtype=\"float32\"):\n",
    "    # Load image into numpy float array\n",
    "    image = np.array(\n",
    "        PIL.Image.open(image_path).convert(\"RGB\").resize((224, 224)), dtype=dtype\n",
    "    )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.4499942e-06, 9.2473765e-06, 2.0949008e-06, 8.7807439e-06,\n",
       "         1.5942234e-05, 7.3268388e-06, 1.2185769e-06, 9.3832878e-06,\n",
       "         7.4934546e-06, 1.7394845e-05, 9.9991953e-01]], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exécution du modèle\n",
    "onnx_input = single_image_pipeline(\n",
    "    \"dataset/tomato/88614302-e6d2-4327-a4fb-a3db9c9ea72e___YLCV_NREC_2861.JPG\"\n",
    ")\n",
    "\n",
    "onnxruntime_outputs = ort_session.run(None, {\"l_x_\": onnx_input})\n",
    "onnxruntime_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Séparation de l'encodeur et du décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceModelEncoder(torch.nn.Module):\n",
    "    def __init__(self, model, scale, mean, std):\n",
    "        super(InferenceModelEncoder, self).__init__()\n",
    "        self.preprocess = torch.nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"reshape\", ReshapeToBatchChannelFirst()),\n",
    "                    (\"normalize\", FixedNormLayer(scale, mean, std)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        self.model = model  # The main model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preprocess(x)  # Apply reshaping and normalization\n",
    "        x = self.model(x)  # Pass to the main model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créé à l'aide de ChatGPT\n",
    "class InferenceModelDecoder(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(InferenceModelDecoder, self).__init__()\n",
    "        self.model = model  # The main model last layer\n",
    "        self.postprocess = torch.nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)  # Pass to the main model\n",
    "        return self.postprocess(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ViT\n",
    "num_labels = 11  # Get number of labels (e.g., 8)\n",
    "\n",
    "model = torchvision.models.vit_b_16(weights=\"IMAGENET1K_V1\")  # Load a pretrained model\n",
    "model.heads.head = torch.nn.Linear(model.heads.head.in_features, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"models/tomato_model_2025_02_28_v2.pt\",\n",
    "        map_location=device,\n",
    "        weights_only=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0183, -0.0336, -0.0224,  ..., -0.0234, -0.0337,  0.0222],\n",
       "        [ 0.0133,  0.0046,  0.0376,  ..., -0.0338, -0.0195, -0.0039],\n",
       "        [ 0.0117,  0.0335, -0.0085,  ...,  0.0160, -0.0332, -0.0282],\n",
       "        ...,\n",
       "        [ 0.0254, -0.0356, -0.0124,  ...,  0.0298,  0.0253, -0.0077],\n",
       "        [ 0.0218,  0.0312, -0.0024,  ..., -0.0325,  0.0179,  0.0299],\n",
       "        [-0.0239, -0.0136,  0.0181,  ...,  0.0369, -0.0023, -0.0013]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract last layer\n",
    "last_layer = torch.nn.Linear(model.heads.head.in_features, num_labels)\n",
    "last_layer.weight = model.heads.head.weight\n",
    "last_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last layer from model\n",
    "model.heads.head = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder and decoder inference models\n",
    "scale = torch.tensor([1 / 256])\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "inference_model_encoder = InferenceModelEncoder(model, scale, mean, std)\n",
    "inference_model_decoder = InferenceModelDecoder(last_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0223, 0.5971, 0.2289, 0.0185, 0.0035, 0.0023, 0.0020, 0.0798, 0.0187,\n",
       "         0.0065, 0.0203]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test inference model encoder and decoder\n",
    "test_input = torch.randn(224, 224, 3)\n",
    "\n",
    "encoded = inference_model_encoder(test_input)\n",
    "decoded = inference_model_decoder(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/onnx/_internal/fx/passes/readability.py:52: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  new_node = self.module.graph.get_attr(normalized_name)\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/fx/graph.py:1586: UserWarning: Node preprocess_normalize_scale target preprocess/normalize/scale preprocess/normalize/scale of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/fx/graph.py:1586: UserWarning: Node preprocess_normalize_mean target preprocess/normalize/mean preprocess/normalize/mean of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/fx/graph.py:1586: UserWarning: Node preprocess_normalize_std target preprocess/normalize/std preprocess/normalize/std of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 37 of general pattern rewrite rules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/.venvs/ai/lib64/python3.11/site-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Export to ONNX\n",
    "onnx_encoder = torch.onnx.dynamo_export(inference_model_encoder, test_input)\n",
    "onnx_encoder.save(\"models/tomato_model_2025_02_28_v2_encoder.onnx\")\n",
    "\n",
    "onnx_decoder = torch.onnx.dynamo_export(inference_model_decoder, encoded)\n",
    "onnx_decoder.save(\"models/tomato_model_2025_02_28_v2_decoder.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Validation de l'exécution du modèle avec ONNX runtime (encoder decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "ort_session_encoder = onnxruntime.InferenceSession(\n",
    "    \"models/tomato_model_2025_02_28_v2_encoder.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "ort_session_decoder = onnxruntime.InferenceSession(\n",
    "    \"models/tomato_model_2025_02_28_v2_decoder.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_pipeline(image_path, dtype=\"float32\"):\n",
    "    # Load image into numpy float array\n",
    "    image = np.array(\n",
    "        PIL.Image.open(image_path).convert(\"RGB\").resize((224, 224)), dtype=dtype\n",
    "    )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.07714486e-01,  3.03961448e-02,  8.70138049e-01,\n",
       "        -8.18296313e-01, -2.94012398e-01,  3.38017076e-01,\n",
       "        -1.66627139e-01,  2.50272781e-01,  9.25461113e-01,\n",
       "        -1.28882423e-01, -4.14819419e-01,  5.26395440e-01,\n",
       "         3.60564232e-01, -9.18536007e-01,  1.99042797e-01,\n",
       "         3.08384933e-03,  4.41153377e-01, -9.58641946e-01,\n",
       "        -7.41269961e-02, -6.60064936e-01, -8.72934222e-01,\n",
       "         1.00470936e+00, -4.35145199e-01,  5.68119168e-01,\n",
       "         4.17836249e-01, -1.07043219e+00,  3.48928981e-02,\n",
       "         8.65398526e-01, -1.04338002e+00,  7.23374724e-01,\n",
       "         1.12749267e+00, -8.93600583e-02, -9.41086709e-01,\n",
       "        -1.05196142e+00,  7.97558546e-01, -3.89004320e-01,\n",
       "         2.09360883e-01,  2.86927879e-01,  8.63989651e-01,\n",
       "        -1.43604144e-01, -1.26661256e-01, -1.56991899e-01,\n",
       "        -6.36127710e-01, -1.38812959e-01,  4.89442796e-01,\n",
       "         6.29252195e-01,  7.34239161e-01, -7.45797306e-02,\n",
       "        -1.43365800e+00,  4.67331916e-01, -7.53821850e-01,\n",
       "        -6.70162976e-01,  1.02852213e+00, -1.13496757e+00,\n",
       "         3.24766755e-01,  5.53462148e-01,  1.58791578e+00,\n",
       "         6.77142367e-02, -7.94435084e-01,  2.36945570e-01,\n",
       "         9.71626341e-01, -3.53949994e-01, -1.43277004e-01,\n",
       "        -1.07344794e+00,  3.75737697e-02, -6.38014078e-01,\n",
       "         5.08531272e-01, -1.30250812e+00,  5.64293087e-01,\n",
       "        -3.29165488e-01, -6.49384737e-01,  1.54316163e+00,\n",
       "         3.48707885e-01, -7.94232711e-02,  4.08519417e-01,\n",
       "        -2.68675178e-01, -1.31432962e+00,  3.99362743e-01,\n",
       "        -1.00660992e+00, -2.06877813e-01,  2.02734202e-01,\n",
       "         1.22810090e+00,  4.49862957e-01, -3.54892731e-01,\n",
       "        -4.02746737e-01,  6.99638844e-01,  6.65887117e-01,\n",
       "        -4.94201034e-01, -1.09640372e+00,  1.33213711e+00,\n",
       "         2.78999388e-01,  6.50178790e-01, -6.25403702e-01,\n",
       "        -1.84429437e-02, -1.80607176e+00,  1.07137799e+00,\n",
       "        -1.89598091e-02,  2.83528090e-01,  1.22256063e-01,\n",
       "         1.19170949e-01,  1.71038225e-01,  1.58198565e-01,\n",
       "         2.48567164e-01,  7.70667911e-01,  6.75795376e-02,\n",
       "        -3.50711972e-01,  4.85951543e-01,  1.08819708e-01,\n",
       "        -2.18748704e-01,  4.62637156e-01, -8.82992089e-01,\n",
       "        -1.66640565e-01, -1.81976068e+00, -1.62447244e-01,\n",
       "        -9.63447869e-01, -6.37868643e-01,  1.07023883e+00,\n",
       "        -3.25960428e-01,  4.26908255e-01,  5.15605330e-01,\n",
       "         8.64998639e-01, -8.61771286e-01, -1.68419987e-01,\n",
       "        -2.48388946e-01, -6.14036322e-01,  2.37330943e-01,\n",
       "         2.24580035e-01, -1.87513530e-01, -7.15805888e-01,\n",
       "        -6.08557403e-01,  4.48584199e-01, -9.27658796e-01,\n",
       "        -4.21915427e-02, -1.14979282e-01,  7.19217241e-01,\n",
       "         6.14950240e-01, -1.11231017e+00,  4.64574248e-01,\n",
       "         1.06398058e+00, -4.64581907e-01, -6.63705528e-01,\n",
       "         4.91339236e-01, -8.35815012e-01,  7.40572631e-01,\n",
       "         8.47434282e-01, -6.33902252e-01,  1.62588567e-01,\n",
       "         1.45595178e-01,  9.88191009e-01, -1.45997012e+00,\n",
       "         2.75481790e-01,  2.14052171e-01, -1.74396229e+00,\n",
       "         7.17804432e-01,  8.34677279e-01, -7.64780879e-01,\n",
       "         7.11582065e-01, -4.16133195e-01, -1.02517974e+00,\n",
       "        -2.71902412e-01, -1.35810971e+00, -1.22455525e+00,\n",
       "         6.12920582e-01, -1.19225241e-01,  1.33340406e+00,\n",
       "         1.87850988e+00, -1.40904620e-01, -7.69195616e-01,\n",
       "         7.95405626e-01,  5.60270905e-01, -2.90524781e-01,\n",
       "         1.25927761e-01,  8.76785696e-01,  3.26017618e-01,\n",
       "        -1.10184394e-01,  9.75553811e-01,  9.67438579e-01,\n",
       "         8.51520777e-01, -1.73586644e-02,  4.85467434e-01,\n",
       "        -4.77020025e-01,  5.96389882e-02,  7.11491048e-01,\n",
       "         1.03169727e+00, -1.15989447e+00,  8.23688135e-02,\n",
       "        -7.22351551e-01,  1.24562085e-01, -2.54514456e-01,\n",
       "         1.26424289e+00,  6.46375358e-01,  6.11390293e-01,\n",
       "        -4.11301732e-01,  4.13277894e-01, -5.14430642e-01,\n",
       "        -5.35857916e-01, -1.58056355e+00, -1.09976165e-01,\n",
       "         5.31880915e-01, -1.69359028e-01,  3.01608235e-01,\n",
       "         4.92446244e-01, -4.46185768e-02,  1.01508185e-01,\n",
       "        -6.40470922e-01,  4.90078539e-01,  8.47645327e-02,\n",
       "        -8.80842209e-01,  9.62690890e-01, -7.17536569e-01,\n",
       "         2.68033832e-01,  3.34250838e-01,  3.02061528e-01,\n",
       "         1.01219451e+00,  4.40674752e-01, -7.54340291e-01,\n",
       "         1.33159161e+00,  1.76233262e-01, -3.56857269e-03,\n",
       "        -7.01469854e-02,  4.18420702e-01, -1.22106120e-01,\n",
       "        -6.49472952e-01, -8.72004449e-01, -2.66984224e-01,\n",
       "        -1.99847028e-01, -5.77115119e-01,  2.13473260e-01,\n",
       "        -5.63289583e-01, -3.67021054e-01,  1.31094718e+00,\n",
       "         1.02180675e-01,  2.00265087e-03, -3.19756269e-01,\n",
       "        -1.11407250e-01, -9.18636501e-01, -6.86113596e-01,\n",
       "        -3.27235192e-01, -4.83928353e-01,  4.96155262e-01,\n",
       "         3.37974072e-01, -5.69085870e-03, -4.06559944e-01,\n",
       "        -2.29661316e-01, -5.41275501e-01,  6.16285324e-01,\n",
       "         1.18508160e+00, -1.55194983e-01, -1.07818931e-01,\n",
       "         3.48817021e-01, -1.50151774e-01,  5.23920774e-01,\n",
       "        -4.80884947e-02, -4.07294691e-01, -6.01872444e-01,\n",
       "        -1.95335388e-01, -3.72423679e-01,  7.04783618e-01,\n",
       "         6.77856922e-01,  9.27444547e-02,  5.82526743e-01,\n",
       "        -9.36190844e-01,  8.72395813e-01,  7.20267713e-01,\n",
       "        -3.15929145e-01, -1.09501982e+00, -2.33510286e-01,\n",
       "         5.44775188e-01,  1.00926340e+00, -3.40549737e-01,\n",
       "         1.07640135e+00,  6.00054383e-01,  6.97900414e-01,\n",
       "         3.14389527e-01, -3.77318710e-02,  8.79518747e-01,\n",
       "        -3.57611269e-01,  2.82133400e-01, -2.11631030e-01,\n",
       "         1.17547238e+00,  1.02055466e+00, -7.01008201e-01,\n",
       "        -5.34856439e-01,  1.03486383e+00,  1.06862926e+00,\n",
       "        -2.80053783e-02, -8.95805478e-01,  8.26368690e-01,\n",
       "        -1.12078881e+00, -2.95537077e-02,  2.54203320e-01,\n",
       "        -4.90741193e-01,  1.88738778e-01,  1.01574726e-01,\n",
       "        -2.01791152e-01, -2.47782812e-01, -1.64334644e-02,\n",
       "         4.44653481e-01,  6.97466731e-01, -1.61247098e+00,\n",
       "        -5.74855566e-01, -1.66385031e+00,  1.16780603e+00,\n",
       "         1.12595606e+00, -2.85130233e-01, -1.11169481e+00,\n",
       "        -2.44633481e-01, -7.81623721e-01, -4.91343260e-01,\n",
       "        -6.64467096e-01,  3.52208376e-01,  2.49924555e-01,\n",
       "        -6.07847512e-01,  4.94844057e-02,  1.90955073e-01,\n",
       "        -2.98112333e-01, -3.94658059e-01, -1.94369569e-01,\n",
       "         8.38672876e-01,  9.65449437e-02, -9.40854371e-01,\n",
       "         1.65289128e+00,  1.46222961e+00, -8.50405812e-01,\n",
       "         2.62614250e-01,  3.56718332e-01, -4.91755754e-02,\n",
       "        -1.60745835e+00,  1.77996770e-01, -1.06534146e-01,\n",
       "         7.81584501e-01, -1.50275469e+00,  7.33633935e-01,\n",
       "         1.59927204e-01,  8.97012532e-01, -8.28496754e-01,\n",
       "        -6.01883054e-01,  1.07125536e-01, -7.72005379e-01,\n",
       "        -1.06821346e+00,  5.68530619e-01, -3.61152232e-01,\n",
       "         7.71932840e-01,  6.34333313e-01,  2.33368114e-01,\n",
       "         4.51009721e-01,  3.52616191e-01,  1.81548387e-01,\n",
       "        -1.78964034e-01,  9.92294192e-01,  1.53814822e-01,\n",
       "         1.18509543e+00,  1.09893477e+00,  1.53181815e+00,\n",
       "        -3.25809836e-01,  1.12649965e+00,  2.46699288e-01,\n",
       "        -5.26393466e-02, -6.60780549e-01, -1.74191207e-01,\n",
       "         1.96127042e-01,  1.75944805e-01,  1.10588825e+00,\n",
       "         5.80414757e-02,  9.72166479e-01,  4.08480406e-01,\n",
       "        -7.56394148e-01, -8.49899292e-01, -8.43931794e-01,\n",
       "         3.11964363e-01, -6.41584873e-01, -1.12411475e+00,\n",
       "         5.14616966e-01, -5.13652384e-01,  6.91243410e-02,\n",
       "        -2.54454255e-01, -1.75507396e-01, -4.84346122e-01,\n",
       "         7.40397573e-01, -1.40436637e+00, -1.24591398e+00,\n",
       "        -1.19173980e+00, -6.11769915e-01, -9.64652561e-03,\n",
       "        -3.71770591e-01, -3.37596446e-01,  4.08069551e-01,\n",
       "        -9.60416019e-01,  3.05246741e-01, -8.17111135e-02,\n",
       "        -1.57855570e-01,  1.26189148e+00,  9.38912809e-01,\n",
       "        -2.26750016e-01, -1.31536834e-02,  3.77148926e-01,\n",
       "         1.52708873e-01, -1.67084706e+00,  5.86596429e-01,\n",
       "        -1.65307418e-01, -4.80762959e-01,  9.08698559e-01,\n",
       "         1.09380376e+00,  3.21322858e-01,  1.17350841e+00,\n",
       "        -4.17351604e-01, -5.75754941e-01, -1.55621612e+00,\n",
       "        -2.62866080e-01, -8.95098746e-01, -1.13614511e+00,\n",
       "        -1.85218334e-01, -8.37834418e-01,  1.08289373e+00,\n",
       "        -1.84008293e-02, -3.74013662e-01,  3.22678685e-01,\n",
       "        -2.29183823e-01,  6.31586686e-02, -2.45787501e-01,\n",
       "        -1.30978048e-01,  2.65563466e-03,  7.73789465e-01,\n",
       "        -5.98292649e-01,  7.04214275e-01,  1.11298466e+00,\n",
       "        -4.59252805e-01,  7.18858778e-01,  1.27645671e+00,\n",
       "         4.11471814e-01, -1.35269165e+00, -4.41063911e-01,\n",
       "         5.28330505e-01,  2.17945075e+00,  1.81553289e-01,\n",
       "         4.93568361e-01,  1.62233040e-01, -7.76014507e-01,\n",
       "        -5.46158135e-01,  9.91900861e-01,  4.20796573e-01,\n",
       "         7.37363338e-01,  7.21236542e-02, -4.75137293e-01,\n",
       "         5.71656406e-01,  2.19130039e-01,  1.13981724e+00,\n",
       "         4.43225540e-02, -3.69414181e-01, -1.39230371e-01,\n",
       "        -6.72526136e-02, -5.54672837e-01,  9.03843641e-01,\n",
       "        -1.00345826e+00, -5.83490849e-01,  2.27843255e-01,\n",
       "         1.30701292e+00,  3.43420655e-01,  3.85057062e-01,\n",
       "        -1.06338251e+00,  4.35223281e-02, -9.82009113e-01,\n",
       "        -1.07608867e+00, -3.98558885e-01, -4.84201759e-02,\n",
       "         1.54086322e-01,  3.94212306e-01, -7.93642402e-01,\n",
       "         1.55690277e+00, -1.22928667e+00,  1.43324032e-01,\n",
       "        -1.04023266e+00, -1.73563027e+00,  7.73105741e-01,\n",
       "        -2.99851030e-01, -8.60811770e-01,  7.47131765e-01,\n",
       "        -1.02405512e+00, -1.56557873e-01,  1.85035005e-01,\n",
       "        -4.43541594e-02,  6.90647960e-01,  4.78408337e-01,\n",
       "        -3.37817580e-01,  1.09445763e+00, -6.55856848e-01,\n",
       "         3.70225042e-01, -6.23082757e-01,  7.40464747e-01,\n",
       "         9.95317161e-01,  4.49063003e-01, -4.21950966e-01,\n",
       "         4.84030545e-01,  5.72482646e-01,  1.61066115e+00,\n",
       "        -1.18713212e+00,  5.95206439e-01,  5.54816425e-01,\n",
       "        -9.09428835e-01,  8.44264507e-01, -2.85520732e-01,\n",
       "         1.92872271e-01, -1.30195379e-01, -1.08016324e+00,\n",
       "        -6.37947917e-01, -1.15313821e-01, -5.40270984e-01,\n",
       "         2.92729020e-01, -4.56988543e-01,  5.50751925e-01,\n",
       "         5.41158468e-02,  3.07176471e-01, -1.09013224e+00,\n",
       "         8.03909600e-01,  3.28006834e-01, -9.41140771e-01,\n",
       "         2.09789693e-01,  2.21428484e-01,  4.64937031e-01,\n",
       "         1.48806977e+00,  2.11455226e-01,  3.15589011e-01,\n",
       "        -7.88373768e-01,  8.09860110e-01,  3.56073022e-01,\n",
       "         6.03842616e-01, -7.72774875e-01,  1.53662845e-01,\n",
       "        -4.37804669e-01,  5.96320868e-01, -5.05119264e-01,\n",
       "        -4.83519226e-01,  6.32265985e-01, -9.53049362e-01,\n",
       "        -9.90050852e-01,  1.88809469e-01,  1.24075249e-01,\n",
       "         3.67214948e-01,  3.36881638e-01,  2.87711136e-02,\n",
       "        -9.74179864e-01, -7.74128497e-01,  3.71041179e-01,\n",
       "         1.41170517e-01,  1.41216859e-01, -5.08933365e-01,\n",
       "        -9.89637613e-01, -8.14208269e-01, -1.02106631e+00,\n",
       "         1.21865797e+00, -3.20482403e-01, -1.06869745e+00,\n",
       "         1.19973540e+00, -3.75763208e-01,  4.01827157e-01,\n",
       "        -9.43629444e-01, -2.84526944e-01,  8.67514193e-01,\n",
       "         8.96539807e-01,  4.43521477e-02,  1.63348711e+00,\n",
       "         2.74312049e-01, -4.47422713e-02, -2.39584655e-01,\n",
       "        -2.18181044e-01, -1.11006320e+00, -2.92209923e-01,\n",
       "        -5.99836469e-01,  8.59964907e-01,  7.70198882e-01,\n",
       "        -1.87941343e-02,  8.77819479e-01, -2.21594572e-02,\n",
       "         4.03870828e-04,  7.44955719e-01, -1.41582382e+00,\n",
       "        -1.20759964e+00, -9.52216029e-01,  8.62923384e-01,\n",
       "         2.17971772e-01,  1.69861540e-02,  2.84667313e-01,\n",
       "        -6.67515218e-01, -2.32937366e-01, -1.42749500e+00,\n",
       "        -9.87537742e-01, -1.30462456e+00, -4.07198042e-01,\n",
       "         1.24683321e+00,  6.20674193e-01,  4.22554225e-01,\n",
       "         1.43956578e+00, -5.62765658e-01, -5.06788373e-01,\n",
       "         3.15194845e-01,  6.54324740e-02,  3.29026766e-03,\n",
       "         7.35646069e-01, -5.22485912e-01,  4.48861361e-01,\n",
       "        -1.45174727e-01,  4.89126116e-01, -7.34110713e-01,\n",
       "        -7.94649839e-01, -3.25291097e-01,  4.99946952e-01,\n",
       "        -1.70921892e-01, -1.23221159e+00,  3.44175756e-01,\n",
       "         2.30440140e-01, -5.96862912e-01, -6.39026463e-01,\n",
       "        -9.91388917e-01, -5.21465421e-01, -1.08326387e+00,\n",
       "        -6.42104030e-01, -1.59479648e-01, -2.06590071e-01,\n",
       "        -7.29736209e-01, -1.82521209e-01, -4.44949865e-01,\n",
       "        -5.57218969e-01,  1.33152559e-01,  5.57961345e-01,\n",
       "         3.06293666e-01, -6.02318227e-01,  1.16198015e+00,\n",
       "        -6.27502263e-01,  6.41660511e-01, -7.21308738e-02,\n",
       "        -1.11230540e+00, -1.23006499e+00,  1.07048474e-01,\n",
       "         9.14471567e-01, -1.52727857e-01,  2.39014000e-01,\n",
       "         8.43645573e-01, -1.83885410e-01, -5.61932087e-01,\n",
       "        -1.47603583e+00,  2.25664258e-01,  6.89425766e-01,\n",
       "        -5.59990048e-01, -7.41115451e-01,  3.14012051e-01,\n",
       "         2.66424805e-01,  1.03633547e+00, -5.01421928e-01,\n",
       "        -4.86119837e-01,  7.62025774e-01, -6.76233411e-01,\n",
       "        -1.00191474e+00, -5.68543077e-01,  6.20104373e-01,\n",
       "         6.61168933e-01, -1.47421086e+00, -3.76017362e-01,\n",
       "        -2.06217870e-01, -8.13827634e-01,  9.83246148e-01,\n",
       "         2.73735911e-01,  1.17965087e-01,  3.22680414e-01,\n",
       "        -8.08479309e-01,  3.50904584e-01, -7.43739232e-02,\n",
       "         9.95196640e-01,  4.79892135e-01, -1.44451427e+00,\n",
       "        -3.24420273e-01, -3.22727039e-02,  9.02337253e-01,\n",
       "         5.01726866e-01, -1.58726901e-01, -2.99766436e-02,\n",
       "        -2.87524372e-01, -8.41476023e-01, -8.84078979e-01,\n",
       "         9.65846598e-01, -5.73637545e-01,  1.93575037e+00,\n",
       "        -1.21756613e+00, -3.34853679e-01,  7.64300406e-01,\n",
       "        -6.19925082e-01, -8.57056379e-01,  4.92973864e-01,\n",
       "        -4.66207296e-01,  1.49969709e+00, -5.67345381e-01,\n",
       "         7.23564267e-01, -9.74490345e-02, -1.85277969e-01,\n",
       "         1.08993024e-01,  6.13992572e-01,  1.04629397e+00,\n",
       "         5.94727337e-01, -2.62306690e-01,  1.16882920e-01,\n",
       "        -8.11815500e-01, -1.92119718e-01,  1.34391177e+00,\n",
       "         1.45511866e+00, -1.54397464e+00,  1.70457995e+00,\n",
       "        -3.45493341e-03,  1.00304353e+00, -9.33958650e-01,\n",
       "         6.15452707e-01,  3.32278341e-01, -1.05525267e+00,\n",
       "         1.31932702e-02, -7.38063574e-01,  1.06816694e-01,\n",
       "         6.52975857e-01,  4.34125423e-01,  2.74379551e-01,\n",
       "        -3.26804012e-01,  4.93598193e-01,  3.45628321e-01,\n",
       "        -7.81022072e-01,  9.25364316e-01,  4.64431226e-01,\n",
       "        -2.50199437e-01, -2.28867292e-01, -4.60351765e-01,\n",
       "        -5.69154680e-01,  7.38974392e-01, -7.80177891e-01,\n",
       "         6.91372812e-01, -2.37994790e-01,  9.26957250e-01,\n",
       "        -5.97516373e-02, -5.77859357e-02, -5.51142514e-01,\n",
       "        -3.91448706e-01, -1.36224166e-01,  1.14810944e+00,\n",
       "        -5.87756574e-01, -8.99019837e-01, -1.96231350e-01,\n",
       "        -4.94348764e-01, -2.16207057e-01,  6.55426085e-01,\n",
       "         4.11687762e-01, -4.54237074e-01, -5.48432134e-02,\n",
       "        -4.41543728e-01,  3.09213012e-01,  6.78566813e-01,\n",
       "        -1.27072215e-01, -2.29374826e-01,  1.45004725e+00,\n",
       "         7.17539132e-01, -1.15454292e+00, -1.14142656e+00,\n",
       "         1.96891233e-01, -3.71219039e-01, -5.96798778e-01,\n",
       "        -2.01417238e-01, -4.23945189e-01,  8.08392644e-01,\n",
       "        -3.72150928e-01,  2.97616541e-01,  4.62427914e-01,\n",
       "         1.30269051e+00,  2.45976761e-01, -1.50978461e-01]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exécution du modèle\n",
    "onnx_input = single_image_pipeline(\n",
    "    \"dataset/tomato/88614302-e6d2-4327-a4fb-a3db9c9ea72e___YLCV_NREC_2861.JPG\"\n",
    ")\n",
    "\n",
    "encoded_image = ort_session_encoder.run(None, {\"l_x_\": onnx_input})[0]\n",
    "encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.4769056e-06, 8.9960467e-06, 2.1273770e-06, 8.8322167e-06,\n",
       "         1.5939237e-05, 7.6271754e-06, 1.2036876e-06, 9.0402264e-06,\n",
       "         7.3717015e-06, 1.6906304e-05, 9.9992037e-01]], dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding\n",
    "decoded_output = ort_session_decoder.run(None, {\"l_x_\": encoded_image})\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
